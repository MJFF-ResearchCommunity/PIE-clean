import sys
import shutil
import logging
from pathlib import Path
import pandas as pd

# Add the parent directory to the Python path to make the pie module importable
sys.path.insert(0, str(Path(__file__).parent.parent))

from pie_clean.biospecimen_loader import *

logging.getLogger("PIE").setLevel(logging.DEBUG)
DATA_DIR = "tests/test_data"
SUB_DIR = "Biospecimen"

def test_load_biospecimen(caplog, tmp_path):
    # This test ensures the function parameters are being applied correctly.
    # The actual content of the return data is tested in the
    # project-specific individual tests below.
    all_data = ['project_151_pQTL_CSF', 'project_151_pQTL_CSF_batch_corrected',
                'metabolomic_lrrk2', 'metabolomic_lrrk2_csf', 'urine_proteomics',
                'project_9000', 'project_222', 'project_196', 'project_177', 'project_214',
                'current_biospecimen', 'blood_chemistry_hematology', 'standard_files']

    # Default call includes everything except standard files
    data = load_biospecimen_data(DATA_DIR)
    assert "Will exclude the following projects: ['standard_files']" in caplog.text
    for data_type in all_data:
        if data_type == "standard_files":
            assert data_type not in data.keys()
        else:
            assert data_type in data.keys()

    # Clear the caplog so we can be sure of what was generated by this call
    caplog.clear()
    # Explicitly exclude nothing
    data = load_biospecimen_data(DATA_DIR, exclude=[])
    assert "Will exclude the following projects" not in caplog.text
    for data_type in all_data:
        assert data_type in data.keys()

    # Clear the caplog so we can be sure of what was generated by this call
    caplog.clear()
    # Explicitly exclude others (includes standard_files)
    data = load_biospecimen_data(DATA_DIR, exclude=["project_222", "project_9000"])
    assert "Will exclude the following projects: ['project_222', 'project_9000']" in caplog.text
    for data_type in all_data:
        if data_type in ["project_222", "project_9000"]:
            assert data_type not in data.keys()
        else:
            assert data_type in data.keys()

    # Test when the directory is empty
    data = load_biospecimen_data(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "Biospecimen directory not found" in caplog.records[-1].message
    assert data == {}

def test_merge_biospecimen(caplog, tmp_path, tmpdir):
    # Test out the handling of function parameters. The actual content of the
    # return data is tested in the project-specific individual tests below.
    pat_list = ["9999", "9999", "9998", "9998"]
    eid_list = ["BL", "V04", "BL", "V04"]
    test_dict = {
            'project_9000': pd.DataFrame(data={"PATNO": pat_list, "EVENT_ID": eid_list, "9000_col": [1, 2, 3, 4]}),
            'project_222': pd.DataFrame(data={"PATNO": pat_list, "EVENT_ID": eid_list, "222_col": [5, 6, 7, 8]}),
            'project_196': pd.DataFrame(data={"PATNO": pat_list, "EVENT_ID": eid_list, "196_col": [9, 10, 11, 12]})
    }

    # Defaults to merge_all=True, and include everything
    df = merge_biospecimen_data(test_dict)
    assert isinstance(df, pd.DataFrame)
    assert df.shape == (4, 5) # 4 initial rows; 3 project cols plus PATNO and EVENT_ID
    assert "project_9000_9000_col" in df.columns.tolist()
    assert "project_222_222_col" in df.columns.tolist()
    assert "project_196_196_col" in df.columns.tolist()

    # Merge all and explicitly include only one dataset
    df = merge_biospecimen_data(test_dict, include=["project_9000"])
    assert isinstance(df, pd.DataFrame)
    assert df.shape == (4, 3) # 4 initial rows; 1 project col plus PATNO and EVENT_ID
    assert "project_9000_9000_col" in df.columns.tolist()
    assert "project_222_222_col" not in df.columns.tolist()
    assert "project_196_196_col" not in df.columns.tolist()

    # Merge all and explicitly exclude one dataset
    df = merge_biospecimen_data(test_dict, exclude=["project_9000"])
    assert isinstance(df, pd.DataFrame)
    assert df.shape == (4, 4) # 4 initial rows; 2 project col plus PATNO and EVENT_ID
    assert "project_9000_9000_col" not in df.columns.tolist()
    assert "project_222_222_col" in df.columns.tolist()
    assert "project_196_196_col" in df.columns.tolist()

    # Don't merge, and include everything
    data = merge_biospecimen_data(test_dict, merge_all=False)
    assert isinstance(data, dict)
    assert data == test_dict # Data is passed right through

    # Don't merge, and explicitly include one dataset
    data = merge_biospecimen_data(test_dict, merge_all=False, include=["project_9000"])
    assert isinstance(data, dict)
    assert "project_9000" in data
    assert "project_222" not in data
    assert "project_196" not in data

    # Don't merge, and explicitly exclude one dataset
    data = merge_biospecimen_data(test_dict, merge_all=False, exclude=["project_9000"])
    assert isinstance(data, dict)
    assert "project_9000" not in data
    assert "project_222" in data
    assert "project_196" in data

    # None of these calls should have written to file, as output_dir defaults to None
    for file in os.listdir("."):
        assert "biospecimen.csv" not in file
    # Now explicitly write to file, when merging data. Use default filename.
    df = merge_biospecimen_data(test_dict, output_dir=tmpdir)
    assert os.path.isfile(f"{tmpdir}/biospecimen.csv")
    # Merge and specify filename
    df = merge_biospecimen_data(test_dict, output_dir=tmpdir, output_filename="test.csv")
    assert os.path.isfile(f"{tmpdir}/test.csv")
    # Don't merge: creates an extra dir, and keys are filenames
    df = merge_biospecimen_data(test_dict, merge_all=False, output_dir=tmpdir)
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_9000.csv")
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_222.csv")
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_196.csv")
    # output_filename has no effect when not merging
    df = merge_biospecimen_data(test_dict, merge_all=False, output_dir=tmpdir, output_filename="test.csv")
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_9000.csv")
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_222.csv")
    assert os.path.isfile(f"{tmpdir}/individual_biospecimen/project_196.csv")

def test_load_project_151(caplog, tmp_path):
    # Defaults to batch_corrected=False
    df = load_project_151_pQTL_CSF(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Project_151" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "2 rows, 8 columns" in caplog.records[-1].message
    assert df.shape[0] == 2 # ...and the output matches the logging
    # PATNOs from each file have been merged together (ints here)
    assert 9999 in df["PATNO"].tolist()
    assert 9998 in df["PATNO"].tolist()
    # Test IDs have become column names, with "151_" prepended
    assert "151_10001-7_3" in df.columns.tolist()
    # Others have been dropped
    assert "PLATEID" not in df.columns.tolist()
    # But SEX was kept
    assert "SEX" in df.columns.tolist()
    # CLINICAL_EVENT has been renamed to EVENT_ID
    assert "CLINICAL_EVENT" not in df.columns.tolist()
    assert "EVENT_ID" in df.columns.tolist()
    assert df["EVENT_ID"].iloc[0] == "BL"
    # And data is complete
    assert not df.isnull().any().any()

    # Next, we haven't mocked the files for batch_corrected=True
    df = load_project_151_pQTL_CSF(f"{DATA_DIR}/{SUB_DIR}", batch_corrected=True)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No batch-corrected Project_151" in caplog.records[-1].message
    assert df.empty

    # Test when the directory is empty
    df = load_project_151_pQTL_CSF(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No non-batch-corrected Project_151_pQTL_in_CSF files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "Project_151_pQTL_in_CSF"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    for n in [1, 2]:
        file = tmp_path / SUB_DIR / f"{testfile}_{n}_of_6_21Test2025.csv"
        tmp = pd.read_csv(file)
        tmp = tmp.drop(columns="PATNO")
        tmp.to_csv(file, index=False)

    df = load_project_151_pQTL_CSF(tmp_path)
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_metabolomic_lrrk2(caplog, tmp_path):
    # First, skip the CSF files
    df = load_metabolomic_lrrk2(f"{DATA_DIR}/{SUB_DIR}", include_csf=False)
    # Logging shows that the files were found correctly
    assert "Successfully processed Metabolomic" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "2 rows, 8 columns" in caplog.records[-1].message
    assert df.shape[0] == 2 # ...and the output matches the logging
    # PATNOs from each file have been merged together (ints here)
    assert 9999 in df["PATNO"].tolist()
    assert 9998 in df["PATNO"].tolist()
    # Test IDs have become column names, with "LRRK2_" prepended
    assert "LRRK2_MZ100.08_RT586.24_pos" in df.columns.tolist()
    # But the CSF tests are not included
    assert "LRRK2_(3-O-sulfo)GalCer(d18:1/16:0)" not in df.columns.tolist()
    # Others have been dropped
    assert "UNITS" not in df.columns.tolist()
    # But SEX was kept
    assert "SEX" in df.columns.tolist()
    # CLINICAL_EVENT has been renamed to EVENT_ID
    assert "CLINICAL_EVENT" not in df.columns.tolist()
    assert "EVENT_ID" in df.columns.tolist()
    assert (df["EVENT_ID"] == "V04").all(), f"Non-V04 EVENT_IDs included: {df['EVENT_ID'].unique()}"
    # And data is complete
    assert not df.isnull().any().any()

    # Next, test the CSF switch defaults to True
    df = load_metabolomic_lrrk2(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Metabolomic" in caplog.records[-1].message
    # Logging shows more rows and more columns
    assert "4 rows, 10 columns" in caplog.records[-1].message
    assert df.shape[0] == 4 # ...and the output matches the logging
    # Now the CSF tests are included, and duplicate TESTNAMEs have UNITs appended
    assert "LRRK2_(3-O-sulfo)GalCer(d18:1/16:0)_adjusted_area_ratio" in df.columns.tolist()
    assert "LRRK2_(3-O-sulfo)GalCer(d18:1/16:0)_area_ratio" in df.columns.tolist()
    # And we have V06 events as well as the original V04s
    assert "V04" in df["EVENT_ID"].tolist()
    assert "V06" in df["EVENT_ID"].tolist()
    # V06 non-CSF tests are null
    assert df[df["EVENT_ID"]=="V04"]["LRRK2_MZ100.08_RT586.24_pos"].notnull().all()
    assert df[df["EVENT_ID"]=="V06"]["LRRK2_MZ100.08_RT586.24_pos"].isnull().all()

    # Test when the directory is empty
    df = load_metabolomic_lrrk2(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No Metabolomic_Analysis_of_LRRK2 files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "Metabolomic_Analysis_of_LRRK2_PD"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    for n in [1, 2]:
        file = tmp_path / SUB_DIR / f"{testfile}_{n}_of_5_21Test2025.csv"
        tmp = pd.read_csv(file)
        tmp = tmp.drop(columns="PATNO")
        tmp.to_csv(file, index=False)

    df = load_metabolomic_lrrk2(tmp_path, include_csf=False)
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_project_9000(caplog, tmp_path):
    # Normal loading
    df = load_project_9000(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Project 9000" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "4 rows, 8 columns" in caplog.records[-1].message
    assert df.shape[0] == 4 # ...and the output matches the logging
    # PATNOs from each file have been merged together (strings here)
    assert "9999" in df["PATNO"].tolist()
    assert "9998" in df["PATNO"].tolist()
    # Test IDs have become column names, with "9000_" and the tissue prepended
    assert "9000_CSF_A1L4H1_SSC5D_LOD" in df.columns.tolist()
    assert "9000_Plasma_A1L4H1_SSC5D_LOD" in df.columns.tolist()
    # Others have been dropped
    assert "OLINKID" not in df.columns.tolist()
    # Project 9000 has EVENT_ID from the start
    assert "BL" in df["EVENT_ID"].tolist()
    assert "V04" in df["EVENT_ID"].tolist()
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_project_9000(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No PPMI_Project_9000 files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "PPMI_Project_9000"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    for n in ["CSF", "Plasma"]:
        file = tmp_path / SUB_DIR / f"{testfile}_{n}_Cardio_NPX_21Test2025.csv"
        tmp = pd.read_csv(file)
        tmp = tmp.drop(columns="PATNO")
        tmp.to_csv(file, index=False)

    df = load_project_9000(tmp_path)
    # Not the last records, but repeated for each failure to find a PATNO
    assert caplog.records[-3].levelname == "ERROR"
    assert "Required columns ['PATNO'] not found" in caplog.records[-3].message
    assert caplog.records[-5].levelname == "ERROR"
    assert "Required columns ['PATNO'] not found" in caplog.records[-5].message
    # And the last record should note that no PATNOs were found at all
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_project_222(caplog, tmp_path):
    # Normal loading
    df = load_project_222(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Project 222" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "4 rows, 14 columns" in caplog.records[-1].message
    assert df.shape[0] == 4 # ...and the output matches the logging
    # PATNOs from each file have been merged together (strings here)
    assert "9999" in df["PATNO"].tolist()
    assert "9998" in df["PATNO"].tolist()
    # Test IDs have become column names, with "222_" and the tissue prepended
    assert "222_CSF_P16860_NPPB_LOD" in df.columns.tolist()
    assert "222_Plasma_O43854_EDIL3_LOD" in df.columns.tolist()
    # Others have been dropped
    assert "OLINKID" not in df.columns.tolist()
    # Project 222 has EVENT_ID from the start
    assert "BL" in df["EVENT_ID"].tolist()
    assert "V04" in df["EVENT_ID"].tolist()
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_project_222(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No PPMI_Project_222 files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "PPMI_Project_222"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    for n in ["CSF", "Plasma"]:
        file = tmp_path / SUB_DIR / f"{testfile}_{n}_Cardio_NPX_21Test2025.csv"
        tmp = pd.read_csv(file)
        tmp = tmp.drop(columns="PATNO")
        tmp.to_csv(file, index=False)

    df = load_project_222(tmp_path)
    # Not the last records, but repeated for each failure to find a PATNO
    assert caplog.records[-3].levelname == "ERROR"
    assert "Required columns ['PATNO'] not found" in caplog.records[-3].message
    assert caplog.records[-5].levelname == "ERROR"
    assert "Required columns ['PATNO'] not found" in caplog.records[-5].message
    # And the last record should note that no PATNOs were found at all
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_project_196(caplog, tmp_path):
    # Normal loading
    df = load_project_196(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Project 196" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "4 rows, 24 columns" in caplog.records[-1].message
    assert df.shape[0] == 4 # ...and the output matches the logging
    # PATNOs from each file have been merged together (strings here)
    assert "9999" in df["PATNO"].tolist()
    assert "9998" in df["PATNO"].tolist()
    # Test IDs have become column names, with "196_" and the tissue prepended
    assert "196_Plasma_O00584_RNASET2_COUNT" in df.columns.tolist()
    assert "196_CSF_O00584_RNASET2_COUNT" in df.columns.tolist()
    # These were both COUNTs files: we should have NPX columns too
    assert "196_Plasma_O43854_EDIL3_NPX" in df.columns.tolist()
    # Others have been dropped
    assert "OLINKID" not in df.columns.tolist()
    # Project 196 has EVENT_ID from the start
    assert "BL" in df["EVENT_ID"].tolist()
    assert "V04" in df["EVENT_ID"].tolist()
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_project_196(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No PPMI_Project_196 files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    for testfile in [
            "PPMI_Project_196_CSF_Cardio_Counts_21Test2025.csv",
            "PPMI_Project_196_Plasma_CARDIO_Counts_21Test2025.csv",
            "PPMI_Project_196_Plasma_Cardio_NPX_21Test2025.csv"]:
        file = tmp_path / SUB_DIR / testfile
        tmp = pd.read_csv(file)
        tmp = tmp.drop(columns="PATNO")
        tmp.to_csv(file, index=False)

    # Clear the caplog so we can be sure of what was generated by this call
    caplog.clear()
    df = load_project_196(tmp_path)
    # Not the last records, but repeated for each failure to find a PATNO
    errors = [r for r in caplog.records if r.levelname == "ERROR"]
    assert len(errors) == 8
    err_type = [r for r in errors if "Error reading PATNO/EVENT_ID" in r.message]
    assert len(err_type) == 3
    err_type = [r for r in errors if "Required columns ['PATNO'] not found" in r.message]
    assert len(err_type) == 3
    # And the last record should note that no PATNOs were found at all
    assert caplog.records[-1].levelname == "ERROR"
    assert "Error merging NPX and Counts data" in caplog.records[-1].message
    assert df.empty

def test_load_project_177(caplog, tmp_path):
    # Normal loading
    df = load_project_177_untargeted_proteomics(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Project 177" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "2 rows, 8 columns" in caplog.records[-1].message
    assert df.shape[0] == 2 # ...and the output matches the logging
    # PATNOs from each file have been merged together (ints here)
    assert 9999 in df["PATNO"].tolist()
    assert 9998 in df["PATNO"].tolist()
    # Test IDs have become column names, with "177_" prepended
    assert "177_P55058" in df.columns.tolist()
    # Others have been dropped
    assert "PLATEID" not in df.columns.tolist()
    # But SEX was kept
    assert "SEX" in df.columns.tolist()
    # CLINICAL_EVENT has been renamed to EVENT_ID
    assert "CLINICAL_EVENT" not in df.columns.tolist()
    assert "EVENT_ID" in df.columns.tolist()
    assert df["EVENT_ID"].iloc[0] == "BL"
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_project_177_untargeted_proteomics(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No PPMI_Project_177 files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "PPMI_Project_177_Untargeted_Proteomics_21TestOct.csv"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    file = tmp_path / SUB_DIR / testfile
    tmp = pd.read_csv(file)
    tmp = tmp.drop(columns="PATNO")
    tmp.to_csv(file, index=False)

    df = load_project_177_untargeted_proteomics(tmp_path)
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_current_biospec(caplog, tmp_path):
    # Normal loading
    df = load_current_biospecimen_analysis(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Current Biospecimen" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "2 rows, 8 columns" in caplog.records[-1].message
    assert df.shape[0] == 2 # ...and the output matches the logging
    # PATNOs from each file have been merged together (ints here)
    assert 9999 in df["PATNO"].tolist()
    assert 9998 in df["PATNO"].tolist()
    # Test IDs have become column names, with "BIO_" prepended
    assert "BIO_SNCA_multiplication" in df.columns.tolist()
    # Different capitalizations of Genotype have been harmonized
    assert "Found inconsistent capitalization" in caplog.text
    assert "BIO_APOE GENOTYPE" in df.columns.tolist()
    assert "BIO_ApoE Genotype" not in df.columns.tolist()
    # The same TESTNAME has been distinguished by appending UNITs
    assert "BIO_APP_Avg CT" in df.columns.tolist()
    assert "BIO_APP_SD" in df.columns.tolist()
    # Others have been dropped
    assert "PLATEID" not in df.columns.tolist()
    # But SEX was kept
    assert "SEX" in df.columns.tolist()
    # CLINICAL_EVENT has been renamed to EVENT_ID
    assert "CLINICAL_EVENT" not in df.columns.tolist()
    assert "EVENT_ID" in df.columns.tolist()
    assert df["EVENT_ID"].iloc[0] == "SC"
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_current_biospecimen_analysis(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No Current_Biospecimen_Analysis_Results files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "Current_Biospecimen_Analysis_Results_21Test2025.csv"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    file = tmp_path / SUB_DIR / testfile
    tmp = pd.read_csv(file)
    tmp = tmp.drop(columns="PATNO")
    tmp.to_csv(file, index=False)

    df = load_current_biospecimen_analysis(tmp_path)
    assert caplog.records[-1].levelname == "ERROR"
    assert "Required column PATNO not found" in caplog.records[-1].message
    assert df.empty

def test_load_blood_chem(caplog, tmp_path):
    # Normal loading
    df = load_blood_chemistry_hematology(f"{DATA_DIR}/{SUB_DIR}")
    # Logging shows that the files were found correctly
    assert "Successfully processed Blood Chemistry" in caplog.records[-1].message
    # Logging shows the data was pivoted into wide format
    assert "2 rows, 11 columns" in caplog.records[-1].message
    assert df.shape[0] == 2 # ...and the output matches the logging
    # PATNOs from each file have been merged together (ints here)
    assert 9999 in df["PATNO"].tolist()
    assert 9998 in df["PATNO"].tolist()
    # Test IDs have become column names, with "BCH_" prepended,
    # and the SI result, high, and low values
    assert "BCH_RCT4_ALT_(SGPT)_LSIRES" in df.columns.tolist()
    assert "BCH_RCT4_ALT_(SGPT)_LSILORNG" in df.columns.tolist()
    assert "BCH_RCT4_ALT_(SGPT)_LSIHIRNG" in df.columns.tolist()
    # Others have been dropped
    assert all(["LUSRES" not in c for c in df.columns.tolist()]), \
            f"US results have not been dropped! Columns are {df.columns.tolist()}"
    # EVENT_ID was there from the start
    assert "EVENT_ID" in df.columns.tolist()
    assert df["EVENT_ID"].iloc[0] == "SC"
    # And data is complete
    assert not df.isnull().any().any()

    # Test when the directory is empty
    df = load_blood_chemistry_hematology(tmp_path)
    assert caplog.records[-1].levelname == "WARNING"
    assert "No Blood_Chemistry___Hematology files" in caplog.records[-1].message
    assert df.empty

    # Test when required column is missing: set up missing PATNO files
    testfile = "Blood_Chemistry___Hematology_21Test2025.csv"
    shutil.copytree(f"{DATA_DIR}/{SUB_DIR}", tmp_path / SUB_DIR, dirs_exist_ok=True)
    file = tmp_path / SUB_DIR / testfile
    tmp = pd.read_csv(file)
    tmp = tmp.drop(columns="PATNO")
    tmp.to_csv(file, index=False)

    df = load_blood_chemistry_hematology(tmp_path)
    assert caplog.records[-2].levelname == "ERROR"
    assert "Required columns ['PATNO'] not found" in caplog.records[-2].message
    assert df.empty
